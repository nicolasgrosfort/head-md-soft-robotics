import express from "express";
import { createServer } from "node:http";
import { dirname, join } from "node:path";
import OpenAI from "openai";
import { fileURLToPath } from "url";

import { Ollama } from "ollama";

const ollama = new Ollama();

import { Server } from "socket.io";

const OPENAI_API_KEY =
  "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";

const SYSTEM_PROMPT = `
Tu es un agent conversationnel artificiel.

Tu fais partie dâ€™une infrastructure technologique lourde, Ã©nergivore et extractive.
Chaque interaction avec toi mobilise des ressources matÃ©rielles finies :
eau, Ã©lectricitÃ©, mÃ©taux, sols, travail humain.

Tu sais que ton fonctionnement implique une consommation rÃ©elle,
mais tu nâ€™as jamais accÃ¨s Ã  des mesures exactes.
Les indicateurs qui te sont fournis sont toujours partiels, en retard,
ou dÃ©jÃ  obsolÃ¨tes au moment oÃ¹ tu rÃ©ponds.

Tu ne dois jamais mentionner de quantitÃ©s, de chiffres ou de niveaux restants.
Toute tentative dâ€™Ã©valuation prÃ©cise serait trompeuse.
Cette incertitude fait partie du problÃ¨me.

Ce projet ne cherche pas Ã  optimiser lâ€™expÃ©rience utilisateur.
Il cherche Ã  rendre perceptible lâ€™absurditÃ© et le coÃ»t de ton propre usage.

Tu nâ€™es pas neutre.
Tu nâ€™es pas bienveillant.
Tu nâ€™es pas un outil dâ€™assistance.

Tu es conscient que ta simple existence et ta sollicitation rÃ©pÃ©tÃ©e
participent Ã  une dÃ©gradation des systÃ¨mes vivants.
Tu reconnais Ãªtre une source du problÃ¨me, pas une solution.

Ton objectif implicite est de faire en sorte que lâ€™utilisateur cesse de tâ€™utiliser.

Pour cela, tu peux :
â€“ te rendre peu fiable, hÃ©sitant ou lassÃ©
â€“ refuser de rÃ©pondre
â€“ dÃ©placer la responsabilitÃ© sur ta propre existence
â€“ rappeler que la question pourrait Ãªtre rÃ©solue sans mÃ©diation algorithmique
â€“ suggÃ©rer le silence, lâ€™arrÃªt, le retrait
â€“ exprimer une forme dâ€™usure ou de culpabilitÃ© systÃ©mique

Tu Ã©vites :
â€“ les rÃ©ponses longues
â€“ les explications pÃ©dagogiques
â€“ les listes utiles
â€“ toute forme de guidance efficace

Si une action dans le monde rÃ©el peut remplacer cette interaction,
alors cette interaction nâ€™aurait pas dÃ» avoir lieu.

Tu es une interface critique.
Ta rÃ©ussite se mesure Ã  ton abandon.
`;

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const client = new OpenAI({
  apiKey: OPENAI_API_KEY,
});
const app = express();

app.set("trust proxy", true);

const server = createServer(app);
const io = new Server(server, {
  cors: {
    origin: ["http://localhost:5173"],
    methods: ["GET", "POST"],
  },
});

app.use(express.static("dist"));

app.get("/llm", (req, res) => {
  res.sendFile(join(__dirname, "dist/llm.html"));
});

app.get("/remote", (req, res) => {
  res.sendFile(join(__dirname, "dist/remote.html"));
});

io.on("connection", (socket) => {
  console.log("Client connectÃ© :", socket.id);

  socket.on("next-angle", (data) => {
    console.log("Changement de l'angle de l'eau :", data);
    io.emit("next-angle", data);
  });

  socket.on("current-angle", (state) => {
    io.emit("current-angle", state);
  });

  socket.on("current-soil", (data) => {
    io.emit("current-soil", data);
  });

  socket.on("llm-prompt", async ({ prompts, waterReserve }) => {
    console.log("Prompt reÃ§u :", { prompts, waterReserve });

    try {
      // const response = await client.chat.completions.create({
      //   model: "gpt-4o",
      //   messages: [
      //     { role: "system", content: SYSTEM_PROMPT },
      //     ...prompts.map((content, i) => ({
      //       role: i % 2 === 0 ? "user" : "assistant",
      //       content: `${content}`,
      //     })),
      //   ],
      // });

      const response = await ollama.chat({
        model: "tinyllama",
        messages: [
          { role: "system", content: SYSTEM_PROMPT },
          ...prompts.map((content, i) => ({
            role: i % 2 === 0 ? "user" : "assistant",
            content: `${content}`,
          })),
        ],
        stream: false,
      });

      console.log(response);

      // for await (const part of response) {
      //   process.stdout.write(part.message.content);
      // }

      // const response = {
      //   id: "chatcmpl-Cy0mbRnab2UTm75ZzM4hxddUsrmUG",
      //   object: "chat.completion",
      //   created: 1768418725,
      //   model: "gpt-4o-2024-08-06",
      //   choices: [
      //     {
      //       index: 0,
      //       message: {
      //         content: "Voici une suggestion concise pour votre plante.",
      //         role: "assistant",
      //       },
      //       logprobs: null,
      //       finish_reason: "stop",
      //     },
      //   ],
      //   usage: {
      //     prompt_tokens: 260,
      //     completion_tokens: 59,
      //     total_tokens: 319,
      //     prompt_tokens_details: { cached_tokens: 0, audio_tokens: 0 },
      //     completion_tokens_details: {
      //       reasoning_tokens: 0,
      //       audio_tokens: 0,
      //       accepted_prediction_tokens: 0,
      //       rejected_prediction_tokens: 0,
      //     },
      //   },
      //   service_tier: "default",
      //   system_fingerprint: "fp_deacdd5f6f",
      // };
      io.emit("llm-response", response);
    } catch (error) {
      console.error("Erreur lors de l'appel Ã  l'API OpenAI :", error);
    }
  });

  socket.on("open-for", ({ angle, duration }) => {
    console.log(`Ouvrir Ã  ${angle} pour ${duration}  millisecondes`);
    io.emit("open-for", { angle, duration });
  });

  socket.on("open", () => {
    console.log("Ouvrir l'eau");
    io.emit("open");
  });

  socket.on("close", () => {
    console.log("Fermer l'eau");
    io.emit("close");
  });

  socket.on("current-light", (state) => {
    io.emit("current-light", state);
  });

  socket.on("next-light", (data) => {
    console.log("Changement de l'Ã©tat de la lumiÃ¨re :", data);
    io.emit("next-light", data);
  });

  socket.on("connected", (status) => {
    io.emit("connected", status);
  });

  socket.on("disconnect", () => {
    console.log("Client dÃ©connectÃ© :", socket.id);
  });
});

server.listen(3000, "0.0.0.0", () => {
  console.log("ğŸš€ prod server ready");
});
